{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ruprompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/skoltech-nlp/russe_detox_2022/main/data/input/train.tsv\n",
    "!wget https://raw.githubusercontent.com/skoltech-nlp/russe_detox_2022/main/data/input/dev.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "df.drop([\"index\"], axis=1, inplace=True)\n",
    "df.to_csv(\"train.tsv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset(\"csv\", data_files={\"train\": \"train.tsv\", \"validation\": \"dev.tsv\"}, sep=\"\\t\")\n",
    "train_dataset = datasets[\"train\"]\n",
    "valid_dataset = datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the backbone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "backbone_id = \"ai-forever/rugpt3large_based_on_gpt2\"\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(backbone_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(backbone_id, pad_token=\"<pad>\", eos_token=\"<pad>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prompt format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruprompts import PromptFormat\n",
    "\n",
    "prompt_format = PromptFormat(\"<P*100>{toxic_comment}<P*20>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parametrization of trainable embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruprompts import TensorPromptProvider\n",
    "from transformers import set_seed\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "prompt_provider = TensorPromptProvider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compose prompt format and prompt provider into prompt object and apply it to the model and tokenizer, i.e. add special tokens to the tokenizer and modify the layer of input embeddings of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruprompts import Prompt\n",
    "\n",
    "prompt = Prompt(prompt_format, prompt_provider)\n",
    "prompt.patch(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data:\n",
    "1. format the data entries with the specified prompt format\n",
    "2. tokenize the resulting sequences\n",
    "3. truncate the `truncation_field` if sequence length exceeds `max_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruprompts import Text2TextPreprocessor\n",
    "\n",
    "preprocessor = Text2TextPreprocessor(\n",
    "    prompt_format=prompt_format,\n",
    "    tokenizer=tokenizer,\n",
    "    target_field=\"neutral_comment1\",\n",
    "    max_tokens=1792,\n",
    "    truncation_field=\"toxic_comment\",\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(preprocessor)\n",
    "valid_dataset = valid_dataset.map(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    learning_rate=0.1,\n",
    "    max_steps=100000,\n",
    "    report_to=\"tensorboard\",\n",
    "    # report_to=[\"tensorboard\", \"wandb\"],  # uncomment to log to WandB\n",
    "    logging_dir=\"logs\",\n",
    "    seed=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose optimization options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(prompt_provider.parameters(), lr=training_args.learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=2000,\n",
    "    num_training_steps=training_args.max_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the callbacks and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from ruprompts.callbacks import (\n",
    "    FreezeTransformerUnfreezePrompt,\n",
    "    ReduceCheckpoint,\n",
    "    SavePretrainedPrompt,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=preprocessor.collate_fn(),\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    callbacks=[FreezeTransformerUnfreezePrompt(), ReduceCheckpoint(), SavePretrainedPrompt(prompt)],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load prompt from the last checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "prompt = Prompt.from_pretrained(f\"./checkpoint-{training_args.max_steps}\")\n",
    "\n",
    "ppln = pipeline(\"text2text-generation-with-prompt\", prompt=prompt, model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppln({\"toxic_comment\": \"Ублюдок, мать твою, а ну иди сюда\"}, do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import transformers\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "beam_count = 10\n",
    "\n",
    "predictions = []\n",
    "    \n",
    "for i in tqdm(valid_dataset[\"toxic_comment\"]):\n",
    "    options = ppln(\n",
    "        {\"toxic_comment\": i},\n",
    "        do_sample=False,\n",
    "        num_beams=beam_count,\n",
    "        num_return_sequences=beam_count,\n",
    "    )\n",
    "\n",
    "    options = [i[\"generated_text\"].replace(\"<pad>\", \"\") for i in options]\n",
    "    answer = sorted(options, key=len)[-1]  # get longest answer\n",
    "    predictions.append(answer)\n",
    "\n",
    "with open(\"subm.txt\", \"w\") as f:\n",
    "    f.writelines(list(map(lambda x: x.replace(\"\\n\", \" \") + \"\\n\", predictions)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
