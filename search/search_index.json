{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ruPrompts is a high-level yet extensible library for fast language model tuning via automatic prompt search, featuring integration with HuggingFace Hub, configuration system powered by Hydra, and command line interface. Prompt is a text instruction for language model, like Translate English to French: cat => For some tasks the prompt is obvious, but for some it isn't. With ruPrompts you can define only the prompt format, like <P*10>{text}<P*10> , and train it automatically for any task, if you have a training dataset. You can currently use ruPrompts for text-to-text tasks, such as summarization, detoxification, style transfer, etc., and for styled text generation, as a special case of text-to-text. Features Modular structure for convenient extensibility Integration with HF Transformers , support for all models with LM head Integration with HF Hub for sharing and loading pretrained prompts CLI and configuration system powered by Hydra Pretrained prompts for ruGPT-3 Installation ruPrompts can be installed with pip : pip install ruprompts [ hydra ] See Installation for other installation options. Usage Loading a pretrained prompt for styled text generation: >>> import ruprompts >>> from transformers import pipeline >>> ppln_joke = pipeline ( \"text-generation-with-prompt\" , prompt = \"konodyuk/prompt_rugpt3large_joke\" ) >>> ppln_joke ( \"\u0413\u043e\u0432\u043e\u0440\u0438\u0442 \u043a\u0440\u0443\u0436\u043a\u0430 \u043b\u043e\u0436\u043a\u0435\" ) [{ \"generated_text\" : '\u0413\u043e\u0432\u043e\u0440\u0438\u0442 \u043a\u0440\u0443\u0436\u043a\u0430 \u043b\u043e\u0436\u043a\u0435: \"\u041d\u0435 \u0431\u043e\u0439\u0441\u044f, \u043d\u0435 \u0443\u0442\u043e\u043d\u0435\u0448\u044c!\".' }] For text2text tasks: >>> ppln_detox = pipeline ( \"text2text-generation-with-prompt\" , prompt = \"konodyuk/prompt_rugpt3large_detox_russe\" ) >>> ppln_detox ( \"\u041e\u043f\u044f\u0442\u044c \u044d\u0442\u0438 \u0442\u0443\u043f\u044b\u0435 \u0434\u044f\u0442\u043b\u044b \u0432\u0441\u0435 \u0438\u0441\u043f\u043e\u0440\u0442\u0438\u043b\u0438, \u0447\u0442\u043e\u0431 \u0438\u0445 \u0447\u0435\u0440\u0442\u0438 \u0432\u0437\u044f\u043b\u0438\" ) [{ \"generated_text\" : '\u041e\u043f\u044f\u0442\u044c \u044d\u0442\u0438 \u043b\u044e\u0434\u0438 \u0432\u0441\u0435 \u0438\u0441\u043f\u043e\u0440\u0442\u0438\u043b\u0438' }] Proceed to Quick Start for a more detailed introduction or start using ruPrompts right now with our Colab Tutorials .","title":"Home"},{"location":"#features","text":"Modular structure for convenient extensibility Integration with HF Transformers , support for all models with LM head Integration with HF Hub for sharing and loading pretrained prompts CLI and configuration system powered by Hydra Pretrained prompts for ruGPT-3","title":"Features"},{"location":"#installation","text":"ruPrompts can be installed with pip : pip install ruprompts [ hydra ] See Installation for other installation options.","title":"Installation"},{"location":"#usage","text":"Loading a pretrained prompt for styled text generation: >>> import ruprompts >>> from transformers import pipeline >>> ppln_joke = pipeline ( \"text-generation-with-prompt\" , prompt = \"konodyuk/prompt_rugpt3large_joke\" ) >>> ppln_joke ( \"\u0413\u043e\u0432\u043e\u0440\u0438\u0442 \u043a\u0440\u0443\u0436\u043a\u0430 \u043b\u043e\u0436\u043a\u0435\" ) [{ \"generated_text\" : '\u0413\u043e\u0432\u043e\u0440\u0438\u0442 \u043a\u0440\u0443\u0436\u043a\u0430 \u043b\u043e\u0436\u043a\u0435: \"\u041d\u0435 \u0431\u043e\u0439\u0441\u044f, \u043d\u0435 \u0443\u0442\u043e\u043d\u0435\u0448\u044c!\".' }] For text2text tasks: >>> ppln_detox = pipeline ( \"text2text-generation-with-prompt\" , prompt = \"konodyuk/prompt_rugpt3large_detox_russe\" ) >>> ppln_detox ( \"\u041e\u043f\u044f\u0442\u044c \u044d\u0442\u0438 \u0442\u0443\u043f\u044b\u0435 \u0434\u044f\u0442\u043b\u044b \u0432\u0441\u0435 \u0438\u0441\u043f\u043e\u0440\u0442\u0438\u043b\u0438, \u0447\u0442\u043e\u0431 \u0438\u0445 \u0447\u0435\u0440\u0442\u0438 \u0432\u0437\u044f\u043b\u0438\" ) [{ \"generated_text\" : '\u041e\u043f\u044f\u0442\u044c \u044d\u0442\u0438 \u043b\u044e\u0434\u0438 \u0432\u0441\u0435 \u0438\u0441\u043f\u043e\u0440\u0442\u0438\u043b\u0438' }] Proceed to Quick Start for a more detailed introduction or start using ruPrompts right now with our Colab Tutorials .","title":"Usage"},{"location":"tutorials/","text":"Notebook Description Inference from pretrained prompt How to load pretrained prompts from HF Hub and run inference Training a prompt with Hydra CLI How to train the prompt on a style transfer task using local data Training a prompt with Python API (RUSSE Detox) RUSSE Detox 2022 baseline Training a prompt with Hydra API (RUSSE Detox) RUSSE Detox 2022 baseline Inference from pretrained prompt (RUSSE Detox) RUSSE Detox 2022 baseline","title":"Tutorials"},{"location":"api/callbacks/","text":"class ruprompts.callbacks.FreezeTransformerUnfreezePrompt Freezes all parameters but those of prompt provider. on_train_begin ( args : TrainingArguments , state : TrainerState , control : TrainerControl , model : PreTrainedModel , ** kwargs ) Event called at the beginning of training. Source code in ruprompts/callbacks.py def on_train_begin ( self , args : TrainingArguments , state : TrainerState , control : TrainerControl , model : PreTrainedModel , ** kwargs , ): for name , param in model . transformer . named_parameters (): if PROMPT_PROVIDER_KEY_NAME in name : param . requires_grad = True else : param . requires_grad = False class ruprompts.callbacks.ReduceCheckpoint Reduces the checkpoint size by keeping only the weights of prompt provider. on_save ( args : TrainingArguments , state : TrainerState , control : TrainerControl , ** kwargs ) Event called after a checkpoint save. Source code in ruprompts/callbacks.py def on_save ( self , args : TrainingArguments , state : TrainerState , control : TrainerControl , ** kwargs ): checkpoint_folder = f \" { PREFIX_CHECKPOINT_DIR } - { state . global_step } \" output_dir = os . path . join ( args . output_dir , checkpoint_folder ) weights_path = os . path . join ( output_dir , WEIGHTS_NAME ) weights = torch . load ( weights_path ) keys_to_remove = [] for weight_key in weights : if PROMPT_PROVIDER_KEY_NAME not in weight_key : keys_to_remove . append ( weight_key ) for key in keys_to_remove : weights . pop ( key ) torch . save ( weights , weights_path ) class ruprompts.callbacks.SavePretrainedPrompt ( prompt : Prompt ) Saves the prompt as pretrained on checkpoint. Parameters: Name Type Description Default prompt Prompt Prompt instance to be saved. required on_save ( args : TrainingArguments , state : TrainerState , control : TrainerControl , ** kwargs ) Event called after a checkpoint save. Source code in ruprompts/callbacks.py def on_save ( self , args : TrainingArguments , state : TrainerState , control : TrainerControl , ** kwargs ): checkpoint_folder = f \" { PREFIX_CHECKPOINT_DIR } - { state . global_step } \" output_dir = os . path . join ( args . output_dir , checkpoint_folder ) self . prompt . save_pretrained ( output_dir ) class ruprompts.callbacks.WBLogHydraConfig ( cfg ) Logs Hydra config to Weights and Biases on training start. Parameters: Name Type Description Default cfg omegaconf.DictConfig Config to be logged. required on_train_begin ( args : TrainingArguments , state : TrainerState , control : TrainerControl , ** kwargs ) Event called at the beginning of training. Source code in ruprompts/callbacks.py def on_train_begin ( self , args : TrainingArguments , state : TrainerState , control : TrainerControl , ** kwargs ): wandb . config . update ({ \"hydra\" : omegaconf . OmegaConf . to_container ( self . cfg )})","title":"Callbacks"},{"location":"api/callbacks/#ruprompts.callbacks.FreezeTransformerUnfreezePrompt","text":"Freezes all parameters but those of prompt provider.","title":"FreezeTransformerUnfreezePrompt"},{"location":"api/callbacks/#ruprompts.callbacks.FreezeTransformerUnfreezePrompt.on_train_begin","text":"Event called at the beginning of training. Source code in ruprompts/callbacks.py def on_train_begin ( self , args : TrainingArguments , state : TrainerState , control : TrainerControl , model : PreTrainedModel , ** kwargs , ): for name , param in model . transformer . named_parameters (): if PROMPT_PROVIDER_KEY_NAME in name : param . requires_grad = True else : param . requires_grad = False","title":"on_train_begin()"},{"location":"api/callbacks/#ruprompts.callbacks.ReduceCheckpoint","text":"Reduces the checkpoint size by keeping only the weights of prompt provider.","title":"ReduceCheckpoint"},{"location":"api/callbacks/#ruprompts.callbacks.ReduceCheckpoint.on_save","text":"Event called after a checkpoint save. Source code in ruprompts/callbacks.py def on_save ( self , args : TrainingArguments , state : TrainerState , control : TrainerControl , ** kwargs ): checkpoint_folder = f \" { PREFIX_CHECKPOINT_DIR } - { state . global_step } \" output_dir = os . path . join ( args . output_dir , checkpoint_folder ) weights_path = os . path . join ( output_dir , WEIGHTS_NAME ) weights = torch . load ( weights_path ) keys_to_remove = [] for weight_key in weights : if PROMPT_PROVIDER_KEY_NAME not in weight_key : keys_to_remove . append ( weight_key ) for key in keys_to_remove : weights . pop ( key ) torch . save ( weights , weights_path )","title":"on_save()"},{"location":"api/callbacks/#ruprompts.callbacks.SavePretrainedPrompt","text":"Saves the prompt as pretrained on checkpoint. Parameters: Name Type Description Default prompt Prompt Prompt instance to be saved. required","title":"SavePretrainedPrompt"},{"location":"api/callbacks/#ruprompts.callbacks.SavePretrainedPrompt.on_save","text":"Event called after a checkpoint save. Source code in ruprompts/callbacks.py def on_save ( self , args : TrainingArguments , state : TrainerState , control : TrainerControl , ** kwargs ): checkpoint_folder = f \" { PREFIX_CHECKPOINT_DIR } - { state . global_step } \" output_dir = os . path . join ( args . output_dir , checkpoint_folder ) self . prompt . save_pretrained ( output_dir )","title":"on_save()"},{"location":"api/callbacks/#ruprompts.callbacks.WBLogHydraConfig","text":"Logs Hydra config to Weights and Biases on training start. Parameters: Name Type Description Default cfg omegaconf.DictConfig Config to be logged. required","title":"WBLogHydraConfig"},{"location":"api/callbacks/#ruprompts.callbacks.WBLogHydraConfig.on_train_begin","text":"Event called at the beginning of training. Source code in ruprompts/callbacks.py def on_train_begin ( self , args : TrainingArguments , state : TrainerState , control : TrainerControl , ** kwargs ): wandb . config . update ({ \"hydra\" : omegaconf . OmegaConf . to_container ( self . cfg )})","title":"on_train_begin()"},{"location":"api/pipelines/","text":"When you run import ruprompts , we add two custom pipelines to transformers : text-generation-with-prompt text2text-generation-with-prompts These pipelines are then accessible with standard syntax: transformers . pipeline ( 'text-generation-with-prompt' , ... ) Read more about pipelines in HF docs . class ruprompts.pipelines.TextGenerationWithPromptPipeline Adds the trained prompt as prefix before passing text to TextGenerationPipeline. Alias: transformers.pipeline('text-generation-with-prompt', ...) . Parameters: Name Type Description Default prompt Prompt or str Prompt used to format input entries. If string is given, loads the prompt with Prompt.from_pretrained . required **kwargs arguments for transformers.Pipeline required Examples: >>> from ruprompts import TextGenerationWithPromptPipeline , Prompt >>> prompt = Prompt . from_pretrained ( ... ) >>> model = AutoLMHeadModel . from_pretrained ( ... ) >>> ppln = TextGenerationWithPromptPipeline ( prompt = prompt , model = model ) >>> from transformers import pipeline >>> ppln = pipeline ( 'text-generation-with-prompt' , prompt = prompt , model = model ) >>> ppln = pipeline ( 'text-generation-with-prompt' , prompt = prompt ) >>> ppln = pipeline ( 'text-generation-with-prompt' , prompt = 'konodyuk/prompt_rugpt3large_joke' ) >>> a = ppln ( text = \"\u0417\u0430\u0445\u043e\u0434\u044f\u0442 \u0432 \u0431\u0430\u0440\" ) >>> b = ppln ( \"\u0417\u0430\u0445\u043e\u0434\u044f\u0442 \u0432 \u0431\u0430\u0440\" ) >>> assert a == b class ruprompts.pipelines.Text2TextGenerationWithPromptPipeline Formats text with the given prompt before passing it to TextGenerationPipeline. Alias: transformers.pipeline('text2text-generation-with-prompt', ...) . Parameters: Name Type Description Default prompt Prompt or str Prompt used to format input entries. If string is given, loads the prompt with Prompt.from_pretrained . required **kwargs arguments for transformers.Pipeline required Examples: >>> from ruprompts import Text2TextGenerationWithPromptPipeline , Prompt >>> prompt = Prompt . from_pretrained ( ... ) >>> model = AutoLMHeadModel . from_pretrained ( ... ) >>> ppln = Text2TextGenerationWithPromptPipeline ( prompt = prompt , model = model ) >>> from transformers import pipeline >>> ppln = pipeline ( 'text2text-generation-with-prompt' , prompt = prompt , model = model ) >>> ppln = pipeline ( 'text2text-generation-with-prompt' , prompt = prompt ) >>> ppln = pipeline ( 'text2text-generation-with-prompt' , prompt = 'konodyuk/prompt_rugpt3large_qa_sberquad' ) >>> ppln = pipeline ( 'text2text-generation-with-prompt' , prompt = 'konodyuk/prompt_rugpt3large_qa_sberquad' ) >>> ppln ( context = \"\u0422\u0440\u0430\u0432\u0430 \u0437\u0435\u043b\u0435\u043d\u0430\u044f.\" , question = \"\u041a\u0430\u043a\u0430\u044f \u0442\u0440\u0430\u0432\u0430?\" ) >>> ppln = pipeline ( 'text2text-generation-with-prompt' , prompt = 'konodyuk/prompt_rugpt3large_detox_russe' ) >>> a = ppln ( text = \"\u041e\u0442\u0432\u0430\u043b\u0438, \u0434\u0443\u0440\u0430\u043a\" ) >>> b = ppln ( \"\u041e\u0442\u0432\u0430\u043b\u0438, \u0434\u0443\u0440\u0430\u043a\" ) >>> assert a == b","title":"Pipelines"},{"location":"api/pipelines/#ruprompts.pipelines.TextGenerationWithPromptPipeline","text":"Adds the trained prompt as prefix before passing text to TextGenerationPipeline. Alias: transformers.pipeline('text-generation-with-prompt', ...) . Parameters: Name Type Description Default prompt Prompt or str Prompt used to format input entries. If string is given, loads the prompt with Prompt.from_pretrained . required **kwargs arguments for transformers.Pipeline required Examples: >>> from ruprompts import TextGenerationWithPromptPipeline , Prompt >>> prompt = Prompt . from_pretrained ( ... ) >>> model = AutoLMHeadModel . from_pretrained ( ... ) >>> ppln = TextGenerationWithPromptPipeline ( prompt = prompt , model = model ) >>> from transformers import pipeline >>> ppln = pipeline ( 'text-generation-with-prompt' , prompt = prompt , model = model ) >>> ppln = pipeline ( 'text-generation-with-prompt' , prompt = prompt ) >>> ppln = pipeline ( 'text-generation-with-prompt' , prompt = 'konodyuk/prompt_rugpt3large_joke' ) >>> a = ppln ( text = \"\u0417\u0430\u0445\u043e\u0434\u044f\u0442 \u0432 \u0431\u0430\u0440\" ) >>> b = ppln ( \"\u0417\u0430\u0445\u043e\u0434\u044f\u0442 \u0432 \u0431\u0430\u0440\" ) >>> assert a == b","title":"TextGenerationWithPromptPipeline"},{"location":"api/pipelines/#ruprompts.pipelines.Text2TextGenerationWithPromptPipeline","text":"Formats text with the given prompt before passing it to TextGenerationPipeline. Alias: transformers.pipeline('text2text-generation-with-prompt', ...) . Parameters: Name Type Description Default prompt Prompt or str Prompt used to format input entries. If string is given, loads the prompt with Prompt.from_pretrained . required **kwargs arguments for transformers.Pipeline required Examples: >>> from ruprompts import Text2TextGenerationWithPromptPipeline , Prompt >>> prompt = Prompt . from_pretrained ( ... ) >>> model = AutoLMHeadModel . from_pretrained ( ... ) >>> ppln = Text2TextGenerationWithPromptPipeline ( prompt = prompt , model = model ) >>> from transformers import pipeline >>> ppln = pipeline ( 'text2text-generation-with-prompt' , prompt = prompt , model = model ) >>> ppln = pipeline ( 'text2text-generation-with-prompt' , prompt = prompt ) >>> ppln = pipeline ( 'text2text-generation-with-prompt' , prompt = 'konodyuk/prompt_rugpt3large_qa_sberquad' ) >>> ppln = pipeline ( 'text2text-generation-with-prompt' , prompt = 'konodyuk/prompt_rugpt3large_qa_sberquad' ) >>> ppln ( context = \"\u0422\u0440\u0430\u0432\u0430 \u0437\u0435\u043b\u0435\u043d\u0430\u044f.\" , question = \"\u041a\u0430\u043a\u0430\u044f \u0442\u0440\u0430\u0432\u0430?\" ) >>> ppln = pipeline ( 'text2text-generation-with-prompt' , prompt = 'konodyuk/prompt_rugpt3large_detox_russe' ) >>> a = ppln ( text = \"\u041e\u0442\u0432\u0430\u043b\u0438, \u0434\u0443\u0440\u0430\u043a\" ) >>> b = ppln ( \"\u041e\u0442\u0432\u0430\u043b\u0438, \u0434\u0443\u0440\u0430\u043a\" ) >>> assert a == b","title":"Text2TextGenerationWithPromptPipeline"},{"location":"api/preprocessing/","text":"dataclass class ruprompts.preprocessing.Text2TextPreprocessor ( prompt_format : BasePromptFormat , tokenizer : PreTrainedTokenizerBase , target_field : str , max_tokens : Optional [ int ] = None , truncation_field : Optional [ str ] = None ) -> None Carries out preprocessing for text2text tasks. Applies prompt format, appends target sequence, tokenizes and truncates each dataset item. Examples: >>> prompt_format = PromptFormat ( \"<P*20> {text} <P*10>\" ) >>> preprocessor = Text2TextPreprocessor ( ... prompt_format = prompt_format , ... tokenizer = tokenizer , ... target_field = \"summary\" , ... max_tokens = 1024 , ... truncation_field = \"text\" ... ) >>> dataset = dataset . map ( preprocessor ) >>> Trainer ( ... , train_dataset = dataset , ... ) Parameters: Name Type Description Default prompt_format BasePromptFormat Prompt format to be applied to dataset items. required tokenizer PreTrainedTokenizerBase required target_field str Target dataset field. required max_tokens Optional[int] Max sequence length in tokens. required truncation_field Optional[str] Field to be truncated when sequence length exceeds max_tokens . required","title":"Preprocessing"},{"location":"api/preprocessing/#ruprompts.preprocessing.Text2TextPreprocessor","text":"Carries out preprocessing for text2text tasks. Applies prompt format, appends target sequence, tokenizes and truncates each dataset item. Examples: >>> prompt_format = PromptFormat ( \"<P*20> {text} <P*10>\" ) >>> preprocessor = Text2TextPreprocessor ( ... prompt_format = prompt_format , ... tokenizer = tokenizer , ... target_field = \"summary\" , ... max_tokens = 1024 , ... truncation_field = \"text\" ... ) >>> dataset = dataset . map ( preprocessor ) >>> Trainer ( ... , train_dataset = dataset , ... ) Parameters: Name Type Description Default prompt_format BasePromptFormat Prompt format to be applied to dataset items. required tokenizer PreTrainedTokenizerBase required target_field str Target dataset field. required max_tokens Optional[int] Max sequence length in tokens. required truncation_field Optional[str] Field to be truncated when sequence length exceeds max_tokens . required","title":"Text2TextPreprocessor"},{"location":"api/prompt/","text":"Prompt is a core class that combines prompt format and prompt provider . It takes care of all the internal modifications that should be applied to model and tokenizer to insert the prompt provider and make it trainable. In particular, when you call Prompt.patch , the following things happen: Underlying prompt format is initialized using the tokenizer. At this step, the <P>Initial manual prompt</P> patterns are tokenized, the prompt format is compiled, and initialization tokens and their positions are identified to be passed to prompt provider . The prompt provider is initialized: it is given the initialization tokens from step 1 and after regular weight initialization overrides them with the embeddings corresponding to the passed intialization tokens. The special tokens needed by prompt format are added to the tokenizer , and their ids are stored. A special drop-in torch.nn.Embedding replacement is initialized with the prompt provider and prompt token ids from step 3. The smart embedding layer from step 4 replaces the default embedding layer in the model . Note Steps 1 and 2 are skipped if the prompt was created with Prompt.from_pretrained . Prompt class also implements sharing methods: Prompt.save_pretrained - saves the trained prompt to disk ot pushes it to HF Hub Prompt.push_to_hub - pushes the trained prompt to HF Hub Prompt.from_pretrained - loads the prompt from disk or HF Hub class ruprompts.prompt.Prompt ( format : BasePromptFormat , provider : BasePromptProvider , config : Optional [ ruprompts . prompt . PromptConfig ] = None ) Core class combining PromptFormat and BasePromptProvider . Implements saving/loading methods and HF hub integration. Examples: >>> p = Prompt ( PromptFormat ( \"<P*50>\" ), TensorPromptProvider ()) >>> p . patch ( model , tokenizer ) >>> trainer . train ( model ) >>> p . save_pretrained ( \"./checkpoint/path\" ) >>> p . push_to_hub ( \"konodyuk/prompt_rugpt3large_detox\" ) >>> p = Prompt . from_pretrained ( \"./checkpoint/path\" ) >>> p . patch ( model , tokenizer ) >>> a = p ( toxic = \"...\" ) >>> b = p . format ( toxic = \"...\" ) >>> assert a == b >>> p = Prompt . from_pretrained ( \"konodyuk/prompt_rugpt3large_detox\" ) >>> ppln = pipeline ( \"text-generation\" , model = model , tokenizer = tokenizer ) >>> ppln_prompt = pipeline ( \"text-generation-with-prompt\" , prompt = p , model = model , tokenizer = tokenizer ) >>> a = ppln ( p ( \"text\" )) >>> b = ppln_prompt ( \"text\" ) >>> assert a == b Parameters: Name Type Description Default format BasePromptFormat Format used to format text for training and inference and for adding special tokens to the tokenizer. required provider BasePromptProvider Provider used to insert trainable embeddings to the positions defined by prompt format. required classmethod from_pretrained ( pretrained_prompt_name_or_path : Union [ str , os . PathLike ], as_safe : bool = False ) -> Prompt Loads a pretrained prompt from disk or HF Hub. Parameters: Name Type Description Default pretrained_prompt_name_or_path Union[str, os.PathLike] Either a HF Hub identifier ( konodyuk/prompt_rugpt3large_detox ) or path to a directory containing prompt saved with save_pretrained . required as_safe bool Whether to load prompt format as PromptFormat or PromptFormatSafe . False Returns: Type Description Prompt Prompt : Pretrained prompt instance. Source code in ruprompts/prompt.py @classmethod def from_pretrained ( cls , pretrained_prompt_name_or_path : Union [ str , os . PathLike ], as_safe : bool = False ) -> \"Prompt\" : \"\"\"Loads a pretrained prompt from disk or HF Hub. Args: pretrained_prompt_name_or_path: Either a HF Hub identifier (`konodyuk/prompt_rugpt3large_detox`) or path to a directory containing prompt saved with :s:`ruprompts.prompt.Prompt.save_pretrained`. as_safe: Whether to load prompt format as :s:`ruprompts.prompt_format.PromptFormat` or :s:`ruprompts.prompt_format.PromptFormatSafe`. Returns: # !s!`ruprompts.prompt.Prompt`: Pretrained prompt instance. \"\"\" if os . path . isdir ( pretrained_prompt_name_or_path ): prompt_file = os . path . join ( pretrained_prompt_name_or_path , PROMPT_FILE_NAME ) prompt_provider_file = os . path . join ( pretrained_prompt_name_or_path , PROMPT_PROVIDER_FILE_NAME ) else : prompt_file = _resolve_file ( pretrained_prompt_name_or_path , PROMPT_FILE_NAME ) prompt_provider_file = _resolve_file ( pretrained_prompt_name_or_path , PROMPT_PROVIDER_FILE_NAME ) with open ( prompt_file , \"r\" ) as f : prompt_dict = json . load ( f ) prompt_format_cls = PromptFormat if as_safe : prompt_format_cls = PromptFormatSafe prompt_format = prompt_format_cls ( ** prompt_dict [ \"format\" ]) prompt_config = PromptConfig ( ** prompt_dict . get ( \"config\" , {})) with open ( prompt_provider_file , \"rb\" ) as f : prompt_provider_weights = torch . load ( f ) prompt_provider = TensorPromptProvider . from_pretrained ( prompt_provider_weights ) prompt = cls ( format = prompt_format , provider = prompt_provider , config = prompt_config ) prompt . ctx . is_initialized = True return prompt patch ( model : PreTrainedModel , tokenizer : PreTrainedTokenizerBase ) Applies the prompt to model and tokenizer. Injects the prompt by adding special prompt tokens to the tokenizer and switching input embedding layer of the model with prompt embedding layer that inserts embeddings from prompt provider into the positions defined by special tokens specified in prompt format. Parameters: Name Type Description Default model PreTrainedModel Model to patch. required tokenizer PreTrainedTokenizerBase Tokenizer to patch. required Source code in ruprompts/prompt.py def patch ( self , model : PreTrainedModel , tokenizer : PreTrainedTokenizerBase ): \"\"\"Applies the prompt to model and tokenizer. Injects the prompt by adding special prompt tokens to the tokenizer and switching input embedding layer of the model with prompt embedding layer that inserts embeddings from prompt provider into the positions defined by special tokens specified in prompt format. Args: model: Model to patch. tokenizer: Tokenizer to patch. \"\"\" self . initialize ( model , tokenizer ) self . attach ( model , tokenizer ) save_pretrained ( save_directory : Union [ str , os . PathLike ], push_to_hub : bool = False , ** kwargs ) Save a prompt to a directory, so that it can be re-loaded using the Prompt.from_pretrained class method. Parameters: Name Type Description Default save_directory Union[str, os.PathLike] Directory to which to save. Will be created if it doesn't exist. required push_to_hub bool Whether or not to push your model to the Hugging Face model hub after saving it. False **kwargs Additional key word arguments passed along to the PushToHubMixin.push_to_hub method. {} Source code in ruprompts/prompt.py def save_pretrained ( self , save_directory : Union [ str , os . PathLike ], push_to_hub : bool = False , ** kwargs ): \"\"\" Save a prompt to a directory, so that it can be re-loaded using the :c:`ruprompts.prompt.Prompt.from_pretrained` class method. Args: save_directory: Directory to which to save. Will be created if it doesn't exist. push_to_hub: Whether or not to push your model to the Hugging Face model hub after saving it. !!! warning Using `push_to_hub=True` will synchronize the repository you are pushing to with `save_directory`, which requires `save_directory` to be a local clone of the repo you are pushing to if it's an existing folder. Pass along `temp_dir=True` to use a temporary directory instead. **kwargs: Additional key word arguments passed along to the [`PushToHubMixin.push_to_hub`](https://huggingface.co/docs/transformers/main_classes/model#transformers.file_utils.PushToHubMixin.push_to_hub) method. \"\"\" if not self . is_initialized : raise UserWarning ( \"Prompt should be initialized to be saved\" ) if os . path . isfile ( save_directory ): raise UserWarning ( \"save_directory should be a directory, got file instead\" ) if push_to_hub : commit_message = kwargs . pop ( \"commit_message\" , None ) repo = self . _create_or_get_repo ( save_directory , ** kwargs ) os . makedirs ( save_directory , exist_ok = True ) output_prompt_provider_file = os . path . join ( save_directory , PROMPT_PROVIDER_FILE_NAME ) self . provider . save_pretrained ( output_prompt_provider_file ) output_prompt_file = os . path . join ( save_directory , PROMPT_FILE_NAME ) json . dump ( self . as_dict (), open ( output_prompt_file , \"w\" , encoding = \"utf-8\" )) if push_to_hub : self . _push_to_hub ( repo , commit_message = commit_message ) class ruprompts.prompt.MultiPrompt ( prompts : Optional [ Dict [ str , Union [ ruprompts . prompt . Prompt , str ]]] = None ) Implements serving multiple prompts with one model. Receives a dict of pretrained prompts with string keys. These keys are used to switch formats. Parameters: Name Type Description Default prompts dict with string keys and Prompt values required Examples: >>> mp = MultiPrompt ({ ... \"key1\" : \"path/to/pretrained/prompt1\" , ... \"key2\" : \"hfhub/prompt_id\" , ... \"key3\" : Prompt . from_pretrained ( \"another_hfhub/prompt_id\" ), ... 'key4\": Prompt.from_pretrained(\"/path/to/another/checkpoint\") ... }) >>> mp . patch ( model , tokenizer ) >>> ppln = pipeline ( \"text-generation\" , model = model , tokenizer = tokenizer ) >>> ppln ( mp ( key = \"key2\" , \"Text for second prompt\" )) >>> ppln ( mp ( key = \"key3\" , text = \"Text for third prompt\" )) >>> ppln ( mp ( key = \"key4\" , keyword = \"Keyword for fourth prompt\" ))","title":"Prompt"},{"location":"api/prompt/#ruprompts.prompt.Prompt","text":"Core class combining PromptFormat and BasePromptProvider . Implements saving/loading methods and HF hub integration. Examples: >>> p = Prompt ( PromptFormat ( \"<P*50>\" ), TensorPromptProvider ()) >>> p . patch ( model , tokenizer ) >>> trainer . train ( model ) >>> p . save_pretrained ( \"./checkpoint/path\" ) >>> p . push_to_hub ( \"konodyuk/prompt_rugpt3large_detox\" ) >>> p = Prompt . from_pretrained ( \"./checkpoint/path\" ) >>> p . patch ( model , tokenizer ) >>> a = p ( toxic = \"...\" ) >>> b = p . format ( toxic = \"...\" ) >>> assert a == b >>> p = Prompt . from_pretrained ( \"konodyuk/prompt_rugpt3large_detox\" ) >>> ppln = pipeline ( \"text-generation\" , model = model , tokenizer = tokenizer ) >>> ppln_prompt = pipeline ( \"text-generation-with-prompt\" , prompt = p , model = model , tokenizer = tokenizer ) >>> a = ppln ( p ( \"text\" )) >>> b = ppln_prompt ( \"text\" ) >>> assert a == b Parameters: Name Type Description Default format BasePromptFormat Format used to format text for training and inference and for adding special tokens to the tokenizer. required provider BasePromptProvider Provider used to insert trainable embeddings to the positions defined by prompt format. required","title":"Prompt"},{"location":"api/prompt/#ruprompts.prompt.Prompt.from_pretrained","text":"Loads a pretrained prompt from disk or HF Hub. Parameters: Name Type Description Default pretrained_prompt_name_or_path Union[str, os.PathLike] Either a HF Hub identifier ( konodyuk/prompt_rugpt3large_detox ) or path to a directory containing prompt saved with save_pretrained . required as_safe bool Whether to load prompt format as PromptFormat or PromptFormatSafe . False Returns: Type Description Prompt","title":"from_pretrained()"},{"location":"api/prompt/#ruprompts.prompt.Prompt.from_pretrained--prompt-pretrained-prompt-instance","text":"Source code in ruprompts/prompt.py @classmethod def from_pretrained ( cls , pretrained_prompt_name_or_path : Union [ str , os . PathLike ], as_safe : bool = False ) -> \"Prompt\" : \"\"\"Loads a pretrained prompt from disk or HF Hub. Args: pretrained_prompt_name_or_path: Either a HF Hub identifier (`konodyuk/prompt_rugpt3large_detox`) or path to a directory containing prompt saved with :s:`ruprompts.prompt.Prompt.save_pretrained`. as_safe: Whether to load prompt format as :s:`ruprompts.prompt_format.PromptFormat` or :s:`ruprompts.prompt_format.PromptFormatSafe`. Returns: # !s!`ruprompts.prompt.Prompt`: Pretrained prompt instance. \"\"\" if os . path . isdir ( pretrained_prompt_name_or_path ): prompt_file = os . path . join ( pretrained_prompt_name_or_path , PROMPT_FILE_NAME ) prompt_provider_file = os . path . join ( pretrained_prompt_name_or_path , PROMPT_PROVIDER_FILE_NAME ) else : prompt_file = _resolve_file ( pretrained_prompt_name_or_path , PROMPT_FILE_NAME ) prompt_provider_file = _resolve_file ( pretrained_prompt_name_or_path , PROMPT_PROVIDER_FILE_NAME ) with open ( prompt_file , \"r\" ) as f : prompt_dict = json . load ( f ) prompt_format_cls = PromptFormat if as_safe : prompt_format_cls = PromptFormatSafe prompt_format = prompt_format_cls ( ** prompt_dict [ \"format\" ]) prompt_config = PromptConfig ( ** prompt_dict . get ( \"config\" , {})) with open ( prompt_provider_file , \"rb\" ) as f : prompt_provider_weights = torch . load ( f ) prompt_provider = TensorPromptProvider . from_pretrained ( prompt_provider_weights ) prompt = cls ( format = prompt_format , provider = prompt_provider , config = prompt_config ) prompt . ctx . is_initialized = True return prompt","title":"Prompt: Pretrained prompt instance."},{"location":"api/prompt/#ruprompts.prompt.Prompt.patch","text":"Applies the prompt to model and tokenizer. Injects the prompt by adding special prompt tokens to the tokenizer and switching input embedding layer of the model with prompt embedding layer that inserts embeddings from prompt provider into the positions defined by special tokens specified in prompt format. Parameters: Name Type Description Default model PreTrainedModel Model to patch. required tokenizer PreTrainedTokenizerBase Tokenizer to patch. required Source code in ruprompts/prompt.py def patch ( self , model : PreTrainedModel , tokenizer : PreTrainedTokenizerBase ): \"\"\"Applies the prompt to model and tokenizer. Injects the prompt by adding special prompt tokens to the tokenizer and switching input embedding layer of the model with prompt embedding layer that inserts embeddings from prompt provider into the positions defined by special tokens specified in prompt format. Args: model: Model to patch. tokenizer: Tokenizer to patch. \"\"\" self . initialize ( model , tokenizer ) self . attach ( model , tokenizer )","title":"patch()"},{"location":"api/prompt/#ruprompts.prompt.Prompt.save_pretrained","text":"Save a prompt to a directory, so that it can be re-loaded using the Prompt.from_pretrained class method. Parameters: Name Type Description Default save_directory Union[str, os.PathLike] Directory to which to save. Will be created if it doesn't exist. required push_to_hub bool Whether or not to push your model to the Hugging Face model hub after saving it. False **kwargs Additional key word arguments passed along to the PushToHubMixin.push_to_hub method. {} Source code in ruprompts/prompt.py def save_pretrained ( self , save_directory : Union [ str , os . PathLike ], push_to_hub : bool = False , ** kwargs ): \"\"\" Save a prompt to a directory, so that it can be re-loaded using the :c:`ruprompts.prompt.Prompt.from_pretrained` class method. Args: save_directory: Directory to which to save. Will be created if it doesn't exist. push_to_hub: Whether or not to push your model to the Hugging Face model hub after saving it. !!! warning Using `push_to_hub=True` will synchronize the repository you are pushing to with `save_directory`, which requires `save_directory` to be a local clone of the repo you are pushing to if it's an existing folder. Pass along `temp_dir=True` to use a temporary directory instead. **kwargs: Additional key word arguments passed along to the [`PushToHubMixin.push_to_hub`](https://huggingface.co/docs/transformers/main_classes/model#transformers.file_utils.PushToHubMixin.push_to_hub) method. \"\"\" if not self . is_initialized : raise UserWarning ( \"Prompt should be initialized to be saved\" ) if os . path . isfile ( save_directory ): raise UserWarning ( \"save_directory should be a directory, got file instead\" ) if push_to_hub : commit_message = kwargs . pop ( \"commit_message\" , None ) repo = self . _create_or_get_repo ( save_directory , ** kwargs ) os . makedirs ( save_directory , exist_ok = True ) output_prompt_provider_file = os . path . join ( save_directory , PROMPT_PROVIDER_FILE_NAME ) self . provider . save_pretrained ( output_prompt_provider_file ) output_prompt_file = os . path . join ( save_directory , PROMPT_FILE_NAME ) json . dump ( self . as_dict (), open ( output_prompt_file , \"w\" , encoding = \"utf-8\" )) if push_to_hub : self . _push_to_hub ( repo , commit_message = commit_message )","title":"save_pretrained()"},{"location":"api/prompt/#ruprompts.prompt.MultiPrompt","text":"Implements serving multiple prompts with one model. Receives a dict of pretrained prompts with string keys. These keys are used to switch formats. Parameters: Name Type Description Default prompts dict with string keys and Prompt values required Examples: >>> mp = MultiPrompt ({ ... \"key1\" : \"path/to/pretrained/prompt1\" , ... \"key2\" : \"hfhub/prompt_id\" , ... \"key3\" : Prompt . from_pretrained ( \"another_hfhub/prompt_id\" ), ... 'key4\": Prompt.from_pretrained(\"/path/to/another/checkpoint\") ... }) >>> mp . patch ( model , tokenizer ) >>> ppln = pipeline ( \"text-generation\" , model = model , tokenizer = tokenizer ) >>> ppln ( mp ( key = \"key2\" , \"Text for second prompt\" )) >>> ppln ( mp ( key = \"key3\" , text = \"Text for third prompt\" )) >>> ppln ( mp ( key = \"key4\" , keyword = \"Keyword for fourth prompt\" ))","title":"MultiPrompt"},{"location":"api/prompt_format/","text":"class ruprompts.prompt_format.PromptFormat ( template : str , compiled_template : Optional [ str ] = None , tokenizer : Optional [ transformers . tokenization_utils_base . PreTrainedTokenizerBase ] = None ) Arranges trainable tokens and dataset fields. Format patterns: Repeated tokens: Pattern: <P*{int}> Example: <P*3> Compiled example: <P><P><P> Initialization from phrase: Pattern: <P>{str}</P> Example: <P>Two tokens</P> Compiled example: <P><P> , prompt provider is initialized with embeddings of tokens Two and tokens Examples: >>> PromptProvider ( \"<P*20> {text} <P*10>\" ) >>> PromptProvider ( \"<P>Passage:</P> {passage} <P> \\n Question:</P> {question} <P> \\n Answer:</P>\" ) See also: BasePromptFormat.__call__ Parameters: Name Type Description Default template str See format patterns . required compiled_template Optional[str] Compiled template. None tokenizer Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] Tokenizer to process the <P>Text</P> patterns. None class ruprompts.prompt_format.PromptFormatSafe ( * args , ** kwargs ) class ruprompts.prompt_format.BasePromptFormat Base class for all prompt formats. property readonly prompt_length : int Count of prompt tokens. special __call__ ( items : Union [ Dict [ str , Any ], List [ Dict [ str , Any ]]] = None , return_ranges : bool = False , ** kwargs ) -> Union [ str , Tuple [ str , Dict [ str , slice ]], List [ str ], Tuple [ List [ str ], List [ Dict [ str , slice ]]]] Applies prompt format to either one or multiple items. Takes a either one item or list of them, where item is a dictionary with string keys. Each item is then formatted into a single string, where the keys are inserted the same way as in format string. If return_ranges=True , also returns a dict of slices, for the value of each key in item containing its start and end positions in the resulting string. Examples: >>> f = PromptFormat ( \"<P> {text} <P>\" ) >>> item = { \"text\" : \"one two three\" , \"other\" : \"value\" } >>> s , r = f ( item , return_ranges = True ) >>> assert s == \"<P>one two three<P>\" >>> assert s [ r ] == item [ \"text\" ] >>> f ( text = \"one two three\" , return_ranges = True ) >>> f ([{ \"text\" : \"a\" }, { \"text\" : \"b\" }], return_ranges = True ) Parameters: Name Type Description Default items Union[Dict[str, Any], List[Dict[str, Any]]] Item or list of items. None return_ranges bool Whether to return ranges. False **kwargs Can be used instead of items (see examples). {} Returns: Type Description Condition str formatted string items is a Dict[str, Any] and return_ranges=False Tuple[str, Dict[str, slice]] formatted string and ranges items is a Dict[str, Any] and return_ranges=True List[str] list of formatted strings items is a List[Dict[str, Any]] and return_ranges=False Tuple[List[str], List[Dict[str, slice]]] list of formatted strings and ranges items is a List[Dict[str, Any]] and return_ranges=True Source code in ruprompts/prompt_format.py def __call__ ( self , items : Optional [ Union [ Dict [ str , Any ], List [ Dict [ str , Any ]]]] = None , return_ranges : bool = False , ** kwargs , ) -> Union [ str , Tuple [ str , Dict [ str , slice ]], List [ str ], Tuple [ List [ str ], List [ Dict [ str , slice ]]] ]: \"\"\"Applies prompt format to either one or multiple items. Takes a either one item or list of them, where item is a dictionary with string keys. Each item is then formatted into a single string, where the keys are inserted the same way as in format string. If `return_ranges=True`, also returns a dict of slices, for the value of each key in item containing its start and end positions in the resulting string. Examples: >>> f = PromptFormat(\"<P>{text}<P>\") >>> item = {\"text\": \"one two three\", \"other\": \"value\"} >>> s, r = f(item, return_ranges=True) >>> assert s == \"<P>one two three<P>\" >>> assert s[r] == item[\"text\"] >>> f(text=\"one two three\", return_ranges=True) >>> f([{\"text\": \"a\"}, {\"text\": \"b\"}], return_ranges=True) Args: items: Item or list of items. return_ranges: Whether to return ranges. **kwargs: Can be used instead of `items` (see examples). # Returns: | Type | Description | Condition | | ------------------------------------------ | ------------------------------------ | ------------------------------------------------------------- | | `str` | formatted string | `items` is a `Dict[str, Any]` and `return_ranges=False` | | `Tuple[str, Dict[str, slice]]` | formatted string and ranges | `items` is a `Dict[str, Any]` and `return_ranges=True` | | `List[str]` | list of formatted strings | `items` is a `List[Dict[str, Any]]` and `return_ranges=False` | | `Tuple[List[str], List[Dict[str, slice]]]` | list of formatted strings and ranges | `items` is a `List[Dict[str, Any]]` and `return_ranges=True` | \"\"\" if items is None : items = kwargs if isinstance ( items , list ): return self . batch_format ( items , return_ranges ) return self . format ( items , return_ranges ) as_dict () -> Dict [ str , Any ] Serializes the prompt object as dict. Returns such a dict d that running __init__(**d) results in an identical object. Source code in ruprompts/prompt_format.py @abc . abstractmethod def as_dict ( self ) -> Dict [ str , Any ]: \"\"\"Serializes the prompt object as dict. Returns such a dict `d` that running `__init__(**d)` results in an identical object. \"\"\" batch_format ( items : List [ Dict [ str , Any ]], return_ranges : bool = False ) -> Union [ List [ str ], Tuple [ List [ str ], List [ Dict [ str , slice ]]]] Formats a list of items into strings and possibly returns ranges. Parameters: Name Type Description Default item Items to be formatted. required return_ranges bool Whether to return ranges. False Returns: Type Description Union[List[str], Tuple[List[str], List[Dict[str, slice]]]] Union[List[str], Tuple[List[str], List[Dict[str, slice]]]]: Returns List[str] when return_ranges=False and Tuple[List[str], List[Dict[str, slice]]] when return_ranges=True Source code in ruprompts/prompt_format.py def batch_format ( self , items : List [ Dict [ str , Any ]], return_ranges : bool = False ) -> Union [ List [ str ], Tuple [ List [ str ], List [ Dict [ str , slice ]]]]: \"\"\"Formats a list of items into strings and possibly returns ranges. Args: item: Items to be formatted. return_ranges: Whether to return ranges. Returns: Union[List[str], Tuple[List[str], List[Dict[str, slice]]]]: Returns `List[str]` when `return_ranges=False` and `Tuple[List[str], List[Dict[str, slice]]]` when `return_ranges=True` \"\"\" result = [ self . format ( item , return_ranges = return_ranges ) for item in items ] if return_ranges : result , ranges = list ( zip ( * result )) return list ( result ), list ( ranges ) return result format ( item : Dict [ str , Any ], return_ranges : bool = False ) -> Union [ str , Tuple [ str , Dict [ str , slice ]]] Formats one item into a string and possibly returns ranges. Parameters: Name Type Description Default item Dict[str, Any] Item to be formatted. required return_ranges bool Whether to return ranges. False Returns: Type Description Union[str, Tuple[str, Dict[str, slice]]] Union[str, Tuple[str, Dict[str, slice]]]: Returns str when return_ranges=False and Tuple[str, Dict[str, slice]] when return_ranges=True Source code in ruprompts/prompt_format.py @abc . abstractmethod def format ( self , item : Dict [ str , Any ], return_ranges : bool = False ) -> Union [ str , Tuple [ str , Dict [ str , slice ]]]: \"\"\"Formats one item into a string and possibly returns ranges. Args: item: Item to be formatted. return_ranges: Whether to return ranges. Returns: Union[str, Tuple[str, Dict[str, slice]]]: Returns `str` when `return_ranges=False` and `Tuple[str, Dict[str, slice]]` when `return_ranges=True` \"\"\"","title":"Prompt Format"},{"location":"api/prompt_format/#ruprompts.prompt_format.PromptFormat","text":"Arranges trainable tokens and dataset fields.","title":"PromptFormat"},{"location":"api/prompt_format/#ruprompts.prompt_format.PromptFormat--format-patterns","text":"Repeated tokens: Pattern: <P*{int}> Example: <P*3> Compiled example: <P><P><P> Initialization from phrase: Pattern: <P>{str}</P> Example: <P>Two tokens</P> Compiled example: <P><P> , prompt provider is initialized with embeddings of tokens Two and tokens Examples: >>> PromptProvider ( \"<P*20> {text} <P*10>\" ) >>> PromptProvider ( \"<P>Passage:</P> {passage} <P> \\n Question:</P> {question} <P> \\n Answer:</P>\" ) See also: BasePromptFormat.__call__ Parameters: Name Type Description Default template str See format patterns . required compiled_template Optional[str] Compiled template. None tokenizer Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] Tokenizer to process the <P>Text</P> patterns. None","title":"Format patterns:"},{"location":"api/prompt_format/#ruprompts.prompt_format.PromptFormatSafe","text":"","title":"PromptFormatSafe"},{"location":"api/prompt_format/#ruprompts.prompt_format.BasePromptFormat","text":"Base class for all prompt formats.","title":"BasePromptFormat"},{"location":"api/prompt_format/#ruprompts.prompt_format.BasePromptFormat.prompt_length","text":"Count of prompt tokens.","title":"prompt_length"},{"location":"api/prompt_format/#ruprompts.prompt_format.BasePromptFormat.__call__","text":"Applies prompt format to either one or multiple items. Takes a either one item or list of them, where item is a dictionary with string keys. Each item is then formatted into a single string, where the keys are inserted the same way as in format string. If return_ranges=True , also returns a dict of slices, for the value of each key in item containing its start and end positions in the resulting string. Examples: >>> f = PromptFormat ( \"<P> {text} <P>\" ) >>> item = { \"text\" : \"one two three\" , \"other\" : \"value\" } >>> s , r = f ( item , return_ranges = True ) >>> assert s == \"<P>one two three<P>\" >>> assert s [ r ] == item [ \"text\" ] >>> f ( text = \"one two three\" , return_ranges = True ) >>> f ([{ \"text\" : \"a\" }, { \"text\" : \"b\" }], return_ranges = True ) Parameters: Name Type Description Default items Union[Dict[str, Any], List[Dict[str, Any]]] Item or list of items. None return_ranges bool Whether to return ranges. False **kwargs Can be used instead of items (see examples). {}","title":"__call__()"},{"location":"api/prompt_format/#ruprompts.prompt_format.BasePromptFormat.__call__--returns","text":"Type Description Condition str formatted string items is a Dict[str, Any] and return_ranges=False Tuple[str, Dict[str, slice]] formatted string and ranges items is a Dict[str, Any] and return_ranges=True List[str] list of formatted strings items is a List[Dict[str, Any]] and return_ranges=False Tuple[List[str], List[Dict[str, slice]]] list of formatted strings and ranges items is a List[Dict[str, Any]] and return_ranges=True Source code in ruprompts/prompt_format.py def __call__ ( self , items : Optional [ Union [ Dict [ str , Any ], List [ Dict [ str , Any ]]]] = None , return_ranges : bool = False , ** kwargs , ) -> Union [ str , Tuple [ str , Dict [ str , slice ]], List [ str ], Tuple [ List [ str ], List [ Dict [ str , slice ]]] ]: \"\"\"Applies prompt format to either one or multiple items. Takes a either one item or list of them, where item is a dictionary with string keys. Each item is then formatted into a single string, where the keys are inserted the same way as in format string. If `return_ranges=True`, also returns a dict of slices, for the value of each key in item containing its start and end positions in the resulting string. Examples: >>> f = PromptFormat(\"<P>{text}<P>\") >>> item = {\"text\": \"one two three\", \"other\": \"value\"} >>> s, r = f(item, return_ranges=True) >>> assert s == \"<P>one two three<P>\" >>> assert s[r] == item[\"text\"] >>> f(text=\"one two three\", return_ranges=True) >>> f([{\"text\": \"a\"}, {\"text\": \"b\"}], return_ranges=True) Args: items: Item or list of items. return_ranges: Whether to return ranges. **kwargs: Can be used instead of `items` (see examples). # Returns: | Type | Description | Condition | | ------------------------------------------ | ------------------------------------ | ------------------------------------------------------------- | | `str` | formatted string | `items` is a `Dict[str, Any]` and `return_ranges=False` | | `Tuple[str, Dict[str, slice]]` | formatted string and ranges | `items` is a `Dict[str, Any]` and `return_ranges=True` | | `List[str]` | list of formatted strings | `items` is a `List[Dict[str, Any]]` and `return_ranges=False` | | `Tuple[List[str], List[Dict[str, slice]]]` | list of formatted strings and ranges | `items` is a `List[Dict[str, Any]]` and `return_ranges=True` | \"\"\" if items is None : items = kwargs if isinstance ( items , list ): return self . batch_format ( items , return_ranges ) return self . format ( items , return_ranges )","title":"Returns:"},{"location":"api/prompt_format/#ruprompts.prompt_format.BasePromptFormat.as_dict","text":"Serializes the prompt object as dict. Returns such a dict d that running __init__(**d) results in an identical object. Source code in ruprompts/prompt_format.py @abc . abstractmethod def as_dict ( self ) -> Dict [ str , Any ]: \"\"\"Serializes the prompt object as dict. Returns such a dict `d` that running `__init__(**d)` results in an identical object. \"\"\"","title":"as_dict()"},{"location":"api/prompt_format/#ruprompts.prompt_format.BasePromptFormat.batch_format","text":"Formats a list of items into strings and possibly returns ranges. Parameters: Name Type Description Default item Items to be formatted. required return_ranges bool Whether to return ranges. False Returns: Type Description Union[List[str], Tuple[List[str], List[Dict[str, slice]]]] Union[List[str], Tuple[List[str], List[Dict[str, slice]]]]: Returns List[str] when return_ranges=False and Tuple[List[str], List[Dict[str, slice]]] when return_ranges=True Source code in ruprompts/prompt_format.py def batch_format ( self , items : List [ Dict [ str , Any ]], return_ranges : bool = False ) -> Union [ List [ str ], Tuple [ List [ str ], List [ Dict [ str , slice ]]]]: \"\"\"Formats a list of items into strings and possibly returns ranges. Args: item: Items to be formatted. return_ranges: Whether to return ranges. Returns: Union[List[str], Tuple[List[str], List[Dict[str, slice]]]]: Returns `List[str]` when `return_ranges=False` and `Tuple[List[str], List[Dict[str, slice]]]` when `return_ranges=True` \"\"\" result = [ self . format ( item , return_ranges = return_ranges ) for item in items ] if return_ranges : result , ranges = list ( zip ( * result )) return list ( result ), list ( ranges ) return result","title":"batch_format()"},{"location":"api/prompt_format/#ruprompts.prompt_format.BasePromptFormat.format","text":"Formats one item into a string and possibly returns ranges. Parameters: Name Type Description Default item Dict[str, Any] Item to be formatted. required return_ranges bool Whether to return ranges. False Returns: Type Description Union[str, Tuple[str, Dict[str, slice]]] Union[str, Tuple[str, Dict[str, slice]]]: Returns str when return_ranges=False and Tuple[str, Dict[str, slice]] when return_ranges=True Source code in ruprompts/prompt_format.py @abc . abstractmethod def format ( self , item : Dict [ str , Any ], return_ranges : bool = False ) -> Union [ str , Tuple [ str , Dict [ str , slice ]]]: \"\"\"Formats one item into a string and possibly returns ranges. Args: item: Item to be formatted. return_ranges: Whether to return ranges. Returns: Union[str, Tuple[str, Dict[str, slice]]]: Returns `str` when `return_ranges=False` and `Tuple[str, Dict[str, slice]]` when `return_ranges=True` \"\"\"","title":"format()"},{"location":"api/prompt_provider/","text":"class ruprompts.prompt_provider.TensorPromptProvider ( init : typing_extensions . Literal [ 'random' , 'vocab' ] = 'vocab' ) Directly stores prompt embeddings as a tensor. Parameters: Name Type Description Default init typing_extensions.Literal['random', 'vocab'] Initialization mode. Initializes embbeddings from random embeddings from vocabulary when set to vocab , randomly otherwise. 'vocab' class ruprompts.prompt_provider.LSTMPromptProvider ( hidden_dim : int = - 1 , input_dim : int = - 1 , num_lstm_layers : int = 2 ) Generates prompt embeddings from LSTM and MLP. Parameters: Name Type Description Default hidden_dim int Hidden dim of LSTM. Defaults to embedding dim of backbone when set to -1. -1 input_dim int Input dim of LSTM. Defaults to embedding dim of backbone when set to -1. -1 num_lstm_layers int Number of LSTM layers. 2 class ruprompts.prompt_provider.BasePromptProvider Base class for all prompt providers.","title":"Prompt Provider"},{"location":"api/prompt_provider/#ruprompts.prompt_provider.TensorPromptProvider","text":"Directly stores prompt embeddings as a tensor. Parameters: Name Type Description Default init typing_extensions.Literal['random', 'vocab'] Initialization mode. Initializes embbeddings from random embeddings from vocabulary when set to vocab , randomly otherwise. 'vocab'","title":"TensorPromptProvider"},{"location":"api/prompt_provider/#ruprompts.prompt_provider.LSTMPromptProvider","text":"Generates prompt embeddings from LSTM and MLP. Parameters: Name Type Description Default hidden_dim int Hidden dim of LSTM. Defaults to embedding dim of backbone when set to -1. -1 input_dim int Input dim of LSTM. Defaults to embedding dim of backbone when set to -1. -1 num_lstm_layers int Number of LSTM layers. 2","title":"LSTMPromptProvider"},{"location":"api/prompt_provider/#ruprompts.prompt_provider.BasePromptProvider","text":"Base class for all prompt providers.","title":"BasePromptProvider"},{"location":"getting-started/installation/","text":"Before installation make sure you have PyTorch installed, since ruPrompts ' installer doesn't take your OS and compute platform into account and may install an unsuitable version of PyTorch. Refer to the PyTorch installation page for details. Installation with pip Use the following command to install only the library part of ruPrompts : pip install ruprompts To also install the ruprompts-train entrypoint, add the corresponding extra: pip install ruprompts [ hydra ] Info I you're using zsh, modify the command to escape the square brackets: pip install ruprompts \\[ hydra \\] Installation from source Installation from source may be your option if you wish to have direct access to conf/ directory to modify Hydra config , e.g. to add custom tasks, datasets, etc. In this case clone the repo and install the package in editable mode: git clone https://github.com/sberbank-ai/ru-prompts cd ru-prompts pip install -e . [ hydra ] Poetry Since ruPrompts is built with Poetry , you may prefer to install it in virtual environment: git clone https://github.com/sberbank-ai/ru-prompts cd ru-prompts pip install poetry poetry install -E hydra Since the ruprompts-train entrypoint willl also be installed to the virtualenv, it will be accessible with poetry run ruprompts-train or poetry shell ruprompts-train Although this option may not be convenient for regular usage, it is preferable for development.","title":"Installation"},{"location":"getting-started/installation/#installation-with-pip","text":"Use the following command to install only the library part of ruPrompts : pip install ruprompts To also install the ruprompts-train entrypoint, add the corresponding extra: pip install ruprompts [ hydra ] Info I you're using zsh, modify the command to escape the square brackets: pip install ruprompts \\[ hydra \\]","title":"Installation with pip"},{"location":"getting-started/installation/#installation-from-source","text":"Installation from source may be your option if you wish to have direct access to conf/ directory to modify Hydra config , e.g. to add custom tasks, datasets, etc. In this case clone the repo and install the package in editable mode: git clone https://github.com/sberbank-ai/ru-prompts cd ru-prompts pip install -e . [ hydra ]","title":"Installation from source"},{"location":"getting-started/installation/#poetry","text":"Since ruPrompts is built with Poetry , you may prefer to install it in virtual environment: git clone https://github.com/sberbank-ai/ru-prompts cd ru-prompts pip install poetry poetry install -E hydra Since the ruprompts-train entrypoint willl also be installed to the virtualenv, it will be accessible with poetry run ruprompts-train or poetry shell ruprompts-train Although this option may not be convenient for regular usage, it is preferable for development.","title":"Poetry"},{"location":"getting-started/quick-start/","text":"Installation Install ruPrompts as follows: pip install ruprompts Note Make sure you have the right version of torch. If you don't have it installed when running the above command, an incorrect (CPU/GPU) version may be installed. For more advanced installation options see Installation . Loading a pretrained prompt Let's download a prompt for joke generation with ruGPT-3 Large: >>> import ruprompts >>> from transformers import pipeline >>> ppln_joke = pipeline ( \"text-generation-with-prompt\" , prompt = \"konodyuk/prompt_rugpt3large_joke\" ) >>> ppln_joke ( \"\u0413\u043e\u0432\u043e\u0440\u0438\u0442 \u043a\u0440\u0443\u0436\u043a\u0430 \u043b\u043e\u0436\u043a\u0435\" ) [{ \"generated_text\" : '\u0413\u043e\u0432\u043e\u0440\u0438\u0442 \u043a\u0440\u0443\u0436\u043a\u0430 \u043b\u043e\u0436\u043a\u0435: \"\u041d\u0435 \u0431\u043e\u0439\u0441\u044f, \u043d\u0435 \u0443\u0442\u043e\u043d\u0435\u0448\u044c!\".' }] Tip When the model and tokenizer are not specified, they are inferred from prompt config and loaded automatically. That's it! Prompts can also handle text-to-text tasks with more complex structure, for example question answering prompt takes two keyword arguments: >>> ppln_qa = pipeline ( 'text2text-generation-with-prompt' , prompt = 'konodyuk/prompt_rugpt3large_qa_sberquad' ) >>> ppln_qa ( context = \"\u0422\u0440\u0430\u0432\u0430 \u0437\u0435\u043b\u0435\u043d\u0430\u044f.\" , question = \"\u041a\u0430\u043a\u0430\u044f \u0442\u0440\u0430\u0432\u0430?\" ) [{ \"generated_text\" : '\u0437\u0435\u043b\u0435\u043d\u0430\u044f' }] If you run these code snippets, you'll notice that a separate model is created each time. We can reuse models and tokenizers by passing them as pipeline arguments: from transformers import GPT2LMHeadModel , AutoTokenizer model_id = \"sberbank-ai/rugpt3large_based_on_gpt2\" model = GPT2LMHeadModel . from_pretrained ( model_id ) tokenizer = AutoTokenizer . from_pretrained ( model_id ) ppln_joke = pipeline ( \"text-generation-with-prompt\" , prompt = \"konodyuk/prompt_rugpt3large_joke\" , model = model , tokenizer = tokenizer ) ppln_joke ( ... ) ppln_qa = pipeline ( \"text-generation-with-prompt\" , prompt = \"konodyuk/prompt_rugpt3large_qa_sberquad\" , model = model , tokenizer = tokenizer ) ppln_qa ( ... ) Note This approach still isn't suitable if you want to use multiple prompts simultaneously. See the MultiPrompt for this purpose. Inference API is very simple and doesn't require any understanding of the internals, but if you wish to learn how it works in theory, then proceed to How it works . For a more practical introduction read Walkthrough first. Contents The documentation is organized in the following parts: Getting Started contains installation instructions, simple introduction to practical usage: inference and training, and a theoretical introduction, explaining the essence of the underlying technique Python API describes the public classes and their API Hydra API contains command line reference and configuration schema, as well as brief introduction to Hydra","title":"Quick Start"},{"location":"getting-started/quick-start/#installation","text":"Install ruPrompts as follows: pip install ruprompts Note Make sure you have the right version of torch. If you don't have it installed when running the above command, an incorrect (CPU/GPU) version may be installed. For more advanced installation options see Installation .","title":"Installation"},{"location":"getting-started/quick-start/#loading-a-pretrained-prompt","text":"Let's download a prompt for joke generation with ruGPT-3 Large: >>> import ruprompts >>> from transformers import pipeline >>> ppln_joke = pipeline ( \"text-generation-with-prompt\" , prompt = \"konodyuk/prompt_rugpt3large_joke\" ) >>> ppln_joke ( \"\u0413\u043e\u0432\u043e\u0440\u0438\u0442 \u043a\u0440\u0443\u0436\u043a\u0430 \u043b\u043e\u0436\u043a\u0435\" ) [{ \"generated_text\" : '\u0413\u043e\u0432\u043e\u0440\u0438\u0442 \u043a\u0440\u0443\u0436\u043a\u0430 \u043b\u043e\u0436\u043a\u0435: \"\u041d\u0435 \u0431\u043e\u0439\u0441\u044f, \u043d\u0435 \u0443\u0442\u043e\u043d\u0435\u0448\u044c!\".' }] Tip When the model and tokenizer are not specified, they are inferred from prompt config and loaded automatically. That's it! Prompts can also handle text-to-text tasks with more complex structure, for example question answering prompt takes two keyword arguments: >>> ppln_qa = pipeline ( 'text2text-generation-with-prompt' , prompt = 'konodyuk/prompt_rugpt3large_qa_sberquad' ) >>> ppln_qa ( context = \"\u0422\u0440\u0430\u0432\u0430 \u0437\u0435\u043b\u0435\u043d\u0430\u044f.\" , question = \"\u041a\u0430\u043a\u0430\u044f \u0442\u0440\u0430\u0432\u0430?\" ) [{ \"generated_text\" : '\u0437\u0435\u043b\u0435\u043d\u0430\u044f' }] If you run these code snippets, you'll notice that a separate model is created each time. We can reuse models and tokenizers by passing them as pipeline arguments: from transformers import GPT2LMHeadModel , AutoTokenizer model_id = \"sberbank-ai/rugpt3large_based_on_gpt2\" model = GPT2LMHeadModel . from_pretrained ( model_id ) tokenizer = AutoTokenizer . from_pretrained ( model_id ) ppln_joke = pipeline ( \"text-generation-with-prompt\" , prompt = \"konodyuk/prompt_rugpt3large_joke\" , model = model , tokenizer = tokenizer ) ppln_joke ( ... ) ppln_qa = pipeline ( \"text-generation-with-prompt\" , prompt = \"konodyuk/prompt_rugpt3large_qa_sberquad\" , model = model , tokenizer = tokenizer ) ppln_qa ( ... ) Note This approach still isn't suitable if you want to use multiple prompts simultaneously. See the MultiPrompt for this purpose. Inference API is very simple and doesn't require any understanding of the internals, but if you wish to learn how it works in theory, then proceed to How it works . For a more practical introduction read Walkthrough first.","title":"Loading a pretrained prompt"},{"location":"getting-started/quick-start/#contents","text":"The documentation is organized in the following parts: Getting Started contains installation instructions, simple introduction to practical usage: inference and training, and a theoretical introduction, explaining the essence of the underlying technique Python API describes the public classes and their API Hydra API contains command line reference and configuration schema, as well as brief introduction to Hydra","title":"Contents"},{"location":"hydra/","text":"Hydra is a powerful configuration framework for Python applications, focused on modularity. The generic config layout consists of multiple groups, each containing multiple options, and a root config, composing the resulting config object by choosing one option in each group: conf/ group1/ option1.yaml option2.yaml group2/ option1.yaml option2.yaml config.yaml Example: composed config command config.yaml group1/option2.yaml group2/option1.yaml group1 : field1 : value1 field2 : value2 group2 : key : \"option one in group two\" python3 main.py defaults : - group1 : option2 - group2 : option1 field1 : value1 field2 : value2 key : \"option one in group two\" You can also override single parameters or the entire options on group level via command line arguments: composed config command config.yaml group1/option2.yaml group2/option2.yaml group1 : field1 : other value field2 : value2 group2 : key : \"option two in group two\" python3 main.py group2 = option2 group1.field1 = \"other value\" defaults : - group1 : option2 - group2 : option1 field1 : value1 field2 : value2 key : \"option two in group two\" Read Hydra docs for a more detailed introduction into its features.","title":"Index"},{"location":"hydra/cli/","text":"We provide a ruprompts-train endpoint, that implements the training loop described in Walkthrough , parameterized by the config described in Config Structure . Info Note that the endpoint is not installed by default. To install it, you should add an extra: pip install ruprompts [ hydra ] See Installation for details. The endpoint behaves as a standard Hydra application, and its parameters should be passed in no hyphen format. Examples Training a task defined in Config Structure : ruprompts-train \\ task = detoxification training.run_name = detox-tensor-linear-lr-1e-1 \\ prompt_provider = tensor training.learning_rate = 1e-1 scheduler = linear_schedule_with_warmup Defining task purely in command line: ruprompts-train \\ task = text2text training.run_name = very-custom-run \\ task.task_name = detoxification \\ prompt_provider = tensor \\ +dataset = from_jsonl \\ +dataset.data_files.train = /path/to/train.jsonl \\ +dataset.data_files.validation = /path/to/validation.jsonl \\ prompt_format.template = '\"<P*60>{toxic}<P*20>\"' \\ preprocessing.target_field = polite \\ preprocessing.truncation_field = toxic \\ preprocessing.max_tokens = 1792 or with dataset from HF Hub: ruprompts-train \\ task = text2text training.run_name = summarization-mlsum \\ task.task_name = summarization \\ prompt_provider = tensor \\ +dataset = default \\ +dataset.path = mlsum \\ +dataset.name = ru \\ prompt_format.template = '\"<P*60>{text}<P*20>\"' \\ preprocessing.target_field = summary \\ preprocessing.truncation_field = text \\ preprocessing.max_tokens = 1792 Working directory behaviour By default ruprompts-train creates a new working directory for each run, grouping them by task name and separating debug runs into the debug folder. Thus, for non-debug runs the workdir is switched to ./outputs/{task_name}/{datetime} relatively to the directory where the endpoint was called, and to ./outputs/debug/{task_name}/{datetime} for debug runs respectively. The working directory contains all the logs and checkpoints. For example, loading a prompt after running ruprompts-train will be done with something like Prompt.from_pretrained(\"./outputs/debug/detoxification/20211231_235959\") . However, switching workdir in runtime makes using relative paths in configs (e.g. for data files) tricky and unreliable. For safety use only absolute paths.","title":"Using CLI"},{"location":"hydra/cli/#examples","text":"Training a task defined in Config Structure : ruprompts-train \\ task = detoxification training.run_name = detox-tensor-linear-lr-1e-1 \\ prompt_provider = tensor training.learning_rate = 1e-1 scheduler = linear_schedule_with_warmup Defining task purely in command line: ruprompts-train \\ task = text2text training.run_name = very-custom-run \\ task.task_name = detoxification \\ prompt_provider = tensor \\ +dataset = from_jsonl \\ +dataset.data_files.train = /path/to/train.jsonl \\ +dataset.data_files.validation = /path/to/validation.jsonl \\ prompt_format.template = '\"<P*60>{toxic}<P*20>\"' \\ preprocessing.target_field = polite \\ preprocessing.truncation_field = toxic \\ preprocessing.max_tokens = 1792 or with dataset from HF Hub: ruprompts-train \\ task = text2text training.run_name = summarization-mlsum \\ task.task_name = summarization \\ prompt_provider = tensor \\ +dataset = default \\ +dataset.path = mlsum \\ +dataset.name = ru \\ prompt_format.template = '\"<P*60>{text}<P*20>\"' \\ preprocessing.target_field = summary \\ preprocessing.truncation_field = text \\ preprocessing.max_tokens = 1792","title":"Examples"},{"location":"hydra/cli/#working-directory-behaviour","text":"By default ruprompts-train creates a new working directory for each run, grouping them by task name and separating debug runs into the debug folder. Thus, for non-debug runs the workdir is switched to ./outputs/{task_name}/{datetime} relatively to the directory where the endpoint was called, and to ./outputs/debug/{task_name}/{datetime} for debug runs respectively. The working directory contains all the logs and checkpoints. For example, loading a prompt after running ruprompts-train will be done with something like Prompt.from_pretrained(\"./outputs/debug/detoxification/20211231_235959\") . However, switching workdir in runtime makes using relative paths in configs (e.g. for data files) tricky and unreliable. For safety use only absolute paths.","title":"Working directory behaviour"},{"location":"hydra/config/","text":"Info If you are not familiar with Hydra, please read our short introduction or the Hydra docs . Our config is located in conf/ folder and consists of the following groups: Backbone Path: conf/backbone Default: rugpt3large Description: Defined the name of pretrained model and tokenizer. Options rugpt3small - loads sberbank-ai/rugpt3small_based_on_gpt2 . rugpt3medium - loads sberbank-ai/rugpt3medium_based_on_gpt2 . rugpt3large - loads sberbank-ai/rugpt3large_based_on_gpt2 . Option format pretrained_model_name_or_path : <string> Model Path: conf/model Default: default Description: Creates a model. Options default - loads an AutoLMHeadModel based on backbone option. gpt - the same as default , but loads a GPT2LMHeadModel. Option format An instantiatable config, returning an instance of pretrained model: _target_ : <module>.<callable> arg1 : value1 arg2 : value2 Tokenizer Path: conf/tokenizer Default: autotokenizer Description: Creates a tokenizer. Options autotokenizer - loads tokenizer based on backbone option. rugpt3 - the same as autotokenizer , but also adds missing special tokens. Option format An instantiatable config, returning an instance of pretrained tokenizer: _target_ : <module>.<callable> arg1 : value1 arg2 : value2 Dataset Path: conf/dataset Default: default Description: Loads a dataset dict containing at least train and validation datasets. Options default - loads a dataset dict using datasets.load_dataset function. from_jsonl - inherits from default , allows to load the dataset dict from json files. Required fields: data_files.train and data_files.validation . Usage example: dataset=from_jsonl data_files.train=/path/to/train.jsonl data_files.validation=/path/to/validation.jsonl . Option format An instantiatable config, returning an instance of dataset dict: _target_ : <module>.<callable> arg1 : value1 arg2 : value2 Preprocessing Path: conf/preprocessing Default: text2text Description: Returns an instance of preprocessor . Options text2text - creates an instance of Text2TextPreprocessor . Required fields match those of target class. Option format An instantiatable config, returning an instance of preprocessor: _target_ : <module>.<callable> arg1 : value1 arg2 : value2 Prompt Format Path: conf/prompt_format Default: default Description: Defines the prompt format . Options default - creates an instance of PromptFormat . Option format An instantiatable config, returning an instance of prompt format: _target_ : <module>.<callable> arg1 : value1 arg2 : value2 Prompt Provider Path: conf/prompt_provider Default: tensor Description: Defines the prompt provider . Options tensor - creates an instance of TensorPromptProvider . lstm - creates an instance of LSTMPromptProvider . Option format An instantiatable config, returning an instance of prompt provider: _target_ : <module>.<callable> arg1 : value1 arg2 : value2 Optimizer Path: conf/optimizer Default: adamw Description: Defines the optimizer. Options adamw - creates an instance of AdamW optimizer. Option format An instantiatable config, returning an instance of torch optimizer: _target_ : <module>.<callable> arg1 : value1 arg2 : value2 Scheduler Path: conf/scheduler Default: adamw Description: Defines the learning rate schedule. Options linear_schedule_with_warmup - creates a linear schedule . constant_schedule_with_warmup - creates a constant schedule . Option format An instantiatable config, returning an instance of torch lr scheduler: _target_ : <module>.<callable> arg1 : value1 arg2 : value2 Training arguments Path: conf/training Default: default Description: Defines the training arguments . Options default - creates an instance of TrainingArguments . Option format No other options are assumed. Callbacks Path: conf/callbacks Default: - freeze_transformer_unfreeze_prompt - reduce_checkpoint - save_pretrained_prompt - wb_log_hydra_config Description: Selects the trainer callbacks . Options freeze_transformer_unfreeze_prompt - creates an instance of FreezeTransformerUnfreezePrompt . Freezes the pretrained transformer and unfreezes the prompt provider before training. reduce_checkpoint - creates an instance of ReduceCheckpoint . After each saving reduces the size of saved model by removing all weights but those of prompt provider. save_pretrained_prompt - creates an instance of SavePretrainedPrompt . Saves the trained prompt using Prompt.save_pretrained in each checkpoint. wb_log_hydra_config - creates an instance of WBLogHydraConfig . Logs the composed Hydra config to Weights and Biases before training. Option format An instantiatable config, returning an instance of TrainerCallback : _target_ : <module>.<callable> arg1 : value1 arg2 : value2 Task Path: conf/task Default: default Description: Overrides the parameters of other groups. Options text2text - selects model=gpt , dataset=default and preprocessing=text2text . other configs that inherit text2text - should define required group parameters in their bodies. Option format task_name : detoxification defaults : - text2text - /dataset : from_jsonl dataset : data_files : train : /path/to/train.jsonl validation : /path/to/validation.jsonl prompt_format : template : \"<P*60>{toxic}<P*20>\" preprocessing : target_field : \"polite\" truncation_field : \"toxic\" max_tokens : 1792","title":"Config Structure"},{"location":"hydra/config/#backbone","text":"Path: conf/backbone Default: rugpt3large Description: Defined the name of pretrained model and tokenizer.","title":"Backbone"},{"location":"hydra/config/#options","text":"rugpt3small - loads sberbank-ai/rugpt3small_based_on_gpt2 . rugpt3medium - loads sberbank-ai/rugpt3medium_based_on_gpt2 . rugpt3large - loads sberbank-ai/rugpt3large_based_on_gpt2 .","title":"Options"},{"location":"hydra/config/#option-format","text":"pretrained_model_name_or_path : <string>","title":"Option format"},{"location":"hydra/config/#model","text":"Path: conf/model Default: default Description: Creates a model.","title":"Model"},{"location":"hydra/config/#options_1","text":"default - loads an AutoLMHeadModel based on backbone option. gpt - the same as default , but loads a GPT2LMHeadModel.","title":"Options"},{"location":"hydra/config/#option-format_1","text":"An instantiatable config, returning an instance of pretrained model: _target_ : <module>.<callable> arg1 : value1 arg2 : value2","title":"Option format"},{"location":"hydra/config/#tokenizer","text":"Path: conf/tokenizer Default: autotokenizer Description: Creates a tokenizer.","title":"Tokenizer"},{"location":"hydra/config/#options_2","text":"autotokenizer - loads tokenizer based on backbone option. rugpt3 - the same as autotokenizer , but also adds missing special tokens.","title":"Options"},{"location":"hydra/config/#option-format_2","text":"An instantiatable config, returning an instance of pretrained tokenizer: _target_ : <module>.<callable> arg1 : value1 arg2 : value2","title":"Option format"},{"location":"hydra/config/#dataset","text":"Path: conf/dataset Default: default Description: Loads a dataset dict containing at least train and validation datasets.","title":"Dataset"},{"location":"hydra/config/#options_3","text":"default - loads a dataset dict using datasets.load_dataset function. from_jsonl - inherits from default , allows to load the dataset dict from json files. Required fields: data_files.train and data_files.validation . Usage example: dataset=from_jsonl data_files.train=/path/to/train.jsonl data_files.validation=/path/to/validation.jsonl .","title":"Options"},{"location":"hydra/config/#option-format_3","text":"An instantiatable config, returning an instance of dataset dict: _target_ : <module>.<callable> arg1 : value1 arg2 : value2","title":"Option format"},{"location":"hydra/config/#preprocessing","text":"Path: conf/preprocessing Default: text2text Description: Returns an instance of preprocessor .","title":"Preprocessing"},{"location":"hydra/config/#options_4","text":"text2text - creates an instance of Text2TextPreprocessor . Required fields match those of target class.","title":"Options"},{"location":"hydra/config/#option-format_4","text":"An instantiatable config, returning an instance of preprocessor: _target_ : <module>.<callable> arg1 : value1 arg2 : value2","title":"Option format"},{"location":"hydra/config/#prompt-format","text":"Path: conf/prompt_format Default: default Description: Defines the prompt format .","title":"Prompt Format"},{"location":"hydra/config/#options_5","text":"default - creates an instance of PromptFormat .","title":"Options"},{"location":"hydra/config/#option-format_5","text":"An instantiatable config, returning an instance of prompt format: _target_ : <module>.<callable> arg1 : value1 arg2 : value2","title":"Option format"},{"location":"hydra/config/#prompt-provider","text":"Path: conf/prompt_provider Default: tensor Description: Defines the prompt provider .","title":"Prompt Provider"},{"location":"hydra/config/#options_6","text":"tensor - creates an instance of TensorPromptProvider . lstm - creates an instance of LSTMPromptProvider .","title":"Options"},{"location":"hydra/config/#option-format_6","text":"An instantiatable config, returning an instance of prompt provider: _target_ : <module>.<callable> arg1 : value1 arg2 : value2","title":"Option format"},{"location":"hydra/config/#optimizer","text":"Path: conf/optimizer Default: adamw Description: Defines the optimizer.","title":"Optimizer"},{"location":"hydra/config/#options_7","text":"adamw - creates an instance of AdamW optimizer.","title":"Options"},{"location":"hydra/config/#option-format_7","text":"An instantiatable config, returning an instance of torch optimizer: _target_ : <module>.<callable> arg1 : value1 arg2 : value2","title":"Option format"},{"location":"hydra/config/#scheduler","text":"Path: conf/scheduler Default: adamw Description: Defines the learning rate schedule.","title":"Scheduler"},{"location":"hydra/config/#options_8","text":"linear_schedule_with_warmup - creates a linear schedule . constant_schedule_with_warmup - creates a constant schedule .","title":"Options"},{"location":"hydra/config/#option-format_8","text":"An instantiatable config, returning an instance of torch lr scheduler: _target_ : <module>.<callable> arg1 : value1 arg2 : value2","title":"Option format"},{"location":"hydra/config/#training-arguments","text":"Path: conf/training Default: default Description: Defines the training arguments .","title":"Training arguments"},{"location":"hydra/config/#options_9","text":"default - creates an instance of TrainingArguments .","title":"Options"},{"location":"hydra/config/#option-format_9","text":"No other options are assumed.","title":"Option format"},{"location":"hydra/config/#callbacks","text":"Path: conf/callbacks Default: - freeze_transformer_unfreeze_prompt - reduce_checkpoint - save_pretrained_prompt - wb_log_hydra_config Description: Selects the trainer callbacks .","title":"Callbacks"},{"location":"hydra/config/#options_10","text":"freeze_transformer_unfreeze_prompt - creates an instance of FreezeTransformerUnfreezePrompt . Freezes the pretrained transformer and unfreezes the prompt provider before training. reduce_checkpoint - creates an instance of ReduceCheckpoint . After each saving reduces the size of saved model by removing all weights but those of prompt provider. save_pretrained_prompt - creates an instance of SavePretrainedPrompt . Saves the trained prompt using Prompt.save_pretrained in each checkpoint. wb_log_hydra_config - creates an instance of WBLogHydraConfig . Logs the composed Hydra config to Weights and Biases before training.","title":"Options"},{"location":"hydra/config/#option-format_10","text":"An instantiatable config, returning an instance of TrainerCallback : _target_ : <module>.<callable> arg1 : value1 arg2 : value2","title":"Option format"},{"location":"hydra/config/#task","text":"Path: conf/task Default: default Description: Overrides the parameters of other groups.","title":"Task"},{"location":"hydra/config/#options_11","text":"text2text - selects model=gpt , dataset=default and preprocessing=text2text . other configs that inherit text2text - should define required group parameters in their bodies.","title":"Options"},{"location":"hydra/config/#option-format_11","text":"task_name : detoxification defaults : - text2text - /dataset : from_jsonl dataset : data_files : train : /path/to/train.jsonl validation : /path/to/validation.jsonl prompt_format : template : \"<P*60>{toxic}<P*20>\" preprocessing : target_field : \"polite\" truncation_field : \"toxic\" max_tokens : 1792","title":"Option format"},{"location":"pretrained/","text":"Card Task Dataset Backbone \ud83e\udd17konodyuk/prompt_rugpt3large_detox_russe Detoxification RUSSE 2020 \ud83e\udd17sberbank-ai/rugpt3large_based_on_gpt2 \ud83e\udd17konodyuk/prompt_rugpt3large_title_mlsum Title Generation mlsum \ud83e\udd17sberbank-ai/rugpt3large_based_on_gpt2 \ud83e\udd17konodyuk/prompt_rugpt3large_summarization_mlsum Summarization mlsum \ud83e\udd17sberbank-ai/rugpt3large_based_on_gpt2 \ud83e\udd17konodyuk/prompt_rugpt3large_qa_sberquad Question Answering SberQuAD \ud83e\udd17sberbank-ai/rugpt3large_based_on_gpt2 \ud83e\udd17konodyuk/prompt_rugpt3large_proverb Styled Generation 4k proverbs \ud83e\udd17sberbank-ai/rugpt3large_based_on_gpt2 \ud83e\udd17konodyuk/prompt_rugpt3large_joke Styled Generation 27MB russian jokes \ud83e\udd17sberbank-ai/rugpt3large_based_on_gpt2","title":"Index"}]}